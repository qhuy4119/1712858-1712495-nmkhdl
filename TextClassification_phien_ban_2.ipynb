{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification phien ban 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Srr8VmYNfVH7FO_DU6JxG4qoE3LLaR8N",
      "authorship_tag": "ABX9TyPe12xbuTH9q0RfUAqMksEs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEt493ujN-bt"
      },
      "source": [
        "#basic library\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Install tokenize\n",
        "!pip install pyvi\n",
        "from pyvi import ViTokenizer\n",
        "\n",
        "# import necessary libraries for deep learning\n",
        "import warnings\n",
        "import keras\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from gensim.models.wrappers import FastText\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU1aTjTCOP12"
      },
      "source": [
        "#load data\n",
        "#file output.csv nay sinh ra tu chuong trinh scraping\n",
        "data_path=r'/content/drive/MyDrive/Học tập/Introduce Data Science/Đố án/output.csv'\n",
        "df= pandas.read_csv(data_path)\n",
        "\n",
        "#preprocessing\n",
        "df = df[pandas.notnull(df['postCotent'])]\n",
        "df= df[df['forumName'] != 'Chuyện trò linh tinh™'] \n",
        "df=df[df['forumName'].map(len) >2]\n",
        "\n",
        "labels=set(df['forumName'])\n",
        "\n",
        "data=dict()\n",
        "#Get labels\n",
        "for label in labels:\n",
        "  data[label]=[]\n",
        "for index,row in df.iterrows():\n",
        "  for label in labels:\n",
        "    if row['forumName']==label:\n",
        "      data[label].append(row['postCotent'])\n",
        "\n",
        "for label in labels:\n",
        "  print(label,': ',len(data[label]))\n",
        "\n",
        "#convert label to num\n",
        "def labelToNum(label):\n",
        "  for (index, value) in enumerate(labels):\n",
        "    if value==label:\n",
        "      return index;\n",
        "  return 0;\n",
        "def numToLabel(num):\n",
        "  for (index, value) in enumerate(labels):\n",
        "    if index==num:\n",
        "      return value;\n",
        "  return data[data.keys()[0]];\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzV3X3eMWpbZ"
      },
      "source": [
        "# Data to plot\n",
        "labels=labels\n",
        "countings = []\n",
        "for label in labels:\n",
        "  countings.append(len(data[label]))\n",
        "\n",
        "# Plot\n",
        "plt.pie(countings, labels=labels,\n",
        "autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGkjkmKSOTFT",
        "outputId": "5c767633-5765-4ff9-e7ff-5df18c3b677b"
      },
      "source": [
        "X=list(df['postCotent'])\n",
        "Y=list(df['forumName'])\n",
        "Y=[labelToNum(i) for i in Y]\n",
        "\n",
        "print(X[:10])\n",
        "print(Y[:10])\n",
        "print(Y[:10])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Máy ngoại hình tốt, ko sướt cấn móp. Pin 100%. Máy đã dán ppf, ốp, kính cường lực.\\nGiấy tờ hóa đơn mua còn đầy đủ. Trong tgian bảo hành còn được thay pin miễn phí. \\nMáy đang dùng song song với XR, dư nên bán. Sdt liên hệ 0908 607 675.\\nGiá chốt 7.400.000 vnđ\\n. ', 'Hello mấy thím, nhân hôm không phải trông con giúp vợ, được dịp rảnh rỗi chém gió tí về mấy món đồ công nghệ mình sưu tầm được. Tôi là dân cơ khí nên mê mấy con gia dụng công nghệ vãi chưởng. Mà bị cái, mỗi lần tôi tha 1 món về thì vợ tôi cứ ầm ĩ bên tai. Nhà tôi thì không quá dư dả gì nhưng có con máy nào ra mà tốt thì tôi cũng ráng đi nghía rồi để tiền mua. Tổng cộng tôi đã tậu đc sương sương vài em như robot hút bụi Neato d5, máy rửa chén tự động Xiaomi Viomi, máy lọc không khí Beurer. Nhìn chung thì mấy món tôi mua giá cũng ko quá cao nhưng mà xài cũng đc. Đợt black friday trước, tôi quyết định chơi sang tự vác tiền mua con máy giặt của LG tên AI DD.\\nĐợt tôi mua là cũng gan trời lắm vì vợ nhà không biết, tới khi biết là người ta giao hàng tới rồi vỡ lẽ. Sau 2 tháng dùng thì tôi sẽ làm 1 quả rì viêu là phụ, khoe của là chính nha mấy thím ))\\nVới độ đoảng và lười của cô vợ nhà tôi thì con máy giặt AI DD này quá là hợp lý. Nhờ vác thây vào Điện máy xanh nghe mấy chị tư vấn và về mò google mấy cái thông số kỹ thuật nên giờ tôi sẽ xài chút “vốn” hiểu biết để diễn giải con máy này cho mấy thím.\\nBỏ tiền thì cho đáng nên với tôi giặt sạch thôi là ko đủ, mà nó phải có nhiều tính năng ngầu ngầu vượt trội hơn mấy con khác. Con máy này đc tích hợp trí tuệ nhân tạo AI DD, con chip của nó chứa hơn 20.000 dữ liệu lớn. Lúc nghe là tôi thấy nhiệm màu rồi nên cũng hơi khoái khoái rồi đó.\\nTôi thấy con này siêu hợp với nhà đông người lắm nhen, thường thì giặt đồ vợ tôi phải ngồi phân loại vải cứng với vải mềm để tách ra giặt vì sợ hư đồ. Còn con máy AI DD này thì nó sẽ tự “cảm” được cân nặng và chất liệu của tất cả các loại quần áo trong chung mẻ giặt. Sau đó nó đề xuất cho mình chu trình ổn áp nhất, bảo vệ quần áo nhà mấy thím tốt nhất có thể luôn. Chỉ cần bấm 1 nút thôi là máy nó làm thay cho không phải nghĩ ngợi gì nhiều.\\nNgoài ra thì em nó còn được bonus hẳn 4 cái vòi phun nước. Mỗi lần giặt tôi nhìn nước phun mạnh từ 4 hướng và xoay đều hết lồng giặt là yên tâm quần áo sẽ được giặt sạch, ko bị dính cặn như máy cũ. Nước nó xịt mạnh nên mấy vết bẩn khi tôi ngồi hàn hay va quẹt khi lắp ráp cũng bị đánh bay. Chứ vòi mà phun lèo tèo như máy giặt cũ nhà tôi thì chắc ôm cục tức vô người rồi. Thời gian gIặt của con này chỉ có 39 phút thôi nên nhờ vậy mà vợ em không còn cớ né kèo “phơi đồ” nữa, mừng gần chết luôn mấy thím.\\nThôi kết nhanh ra chơi với con mấy thím ơi, thiết kế của con máy này rất vừa vặn với căn hộ có diện tích nhỏ như nhà tôi, dung tích 10,5kg mà vẫn để đc ngoài ban công nên tôi rất là ưng.\\nTừ ngày có mấy món đồ công nghệ xịn như vậy, vợ tôi cũng hết phàn nàn tốn thời gian làm việc nhà. Nói chớ, ráng để tiền mua đồ gia dụng ngon hịn để đỡ đần việc nhà cho mấy bà vợ. Nhà mấy thím thì tôi ko biết nhưng vợ nhà tôi làm cô giáo nên bận tối mũi với đám học trò. Mua cho ẻm đỡ cực mấy thím ơi.', 'Mời các bác vào tham luận cho vui. cái gì nên or không nên lắp, tốt cho sử dụng lâu dài hay theo sở thích cá nhân\\nEm trước - Tucson\\nMời các bác', 'Xin phép Add duyệt bài !!!\\nGiúp đỡ , tạo điều kiện giúp nhau đi lên ae nhé .\\nVẫn ép kính thay màn hình cho ae Grap , Be, Go jack, shipper nhé mọi người\\niPhone 6 =>> 8plus đồng giá 150k.\\nOppo, Xiaomi , hawei đồng giá 200 ( trừ một số loại máy sử dụng công nghệ màn Amoleg )\\nSamsung từ 200k =>>> .... ( vui lòng liên hệ..\\nThay pin giá gốc cho ae bảo hành 3 tháng.\\nRất mong được ae ghé qua ủng hộ và trải nhiệm dịch vụ.\\nCảm ơn vì đã đọc bài em !!!\\nĐ/c : số nhà 6 ngõ 71 đường Mỹ đình\\nSdt : 0977512226\\nCùng loại kính, các cửa hàng lớn nhiều chi phí dịch vụ: mặt bằng, lợi nhuận .... nhân viên. Em làm trực tiếp tại nhà riêng. Lấy rẻ kiếm cơm.\\nRủi ro cũng có, nhưng cố làm sao ít rủi ro nhất: ít vỡ màn thì vẫn có lãi ít.\\nNhiều cửa hàng. họ còn nhận ép kính. nhưng bóc máy ra lại mang màn vào các cửa hàng chuyên ép kính cho thợ làm. họ ăn chênh. lãi còn nhiều hơn người làm ép kính.', 'Mới nâng cấp máy, cần bán  hàng chính hãng SSVN, bản , 2 sim, hết bảo hành, nguyên bản, hình thức còn rất đẹp, mặt sau dán carbon từ lúc mua, hoạt động hoàn hảo.  fullbox, đủ hộp, sách, xạc, tai nghe zin', \":   và tuitham gia đánh con Beat : \\nXem thêm tại  chính thức tại (Tặng bạn thân bị chém) by by  aka by(Chào sân Event #1)( Feat  )Feat by  by  by  ft (nỗi buồn 60s) by  by by  - by  - by  by  by  (in dreamy challenging) by by  by  (Diss Xà) by  by by   ()by   (Dota 2) cover by cover by K'Leoly - DSK cover by Lệnh Hồ Huynh Đệ\", 'Anh em có thể mua online về thay :\\nTặng kèm bộ mở máy 9in1 ,Bộ này nhỏ gọn mà mở đầy đủ các ốc iphone. Tuy nhiên không thể so với chuyên dụng\\nLazada : \\nSendo : \\nShopee : \\nBảo hành luôn dung lượng sau 500 lần xạc còn hơn 80%, số lần xạc đạt đến mức 800 lần', 'HH shopM thu mua tất cả các loại máy game cũ đã qua sử dụng : PS2 PS3 PS4 PS VITA XBOX360 Wii 3DS NDS IPAD 1 IPAD 2 IPAD 3TAB......vv.........\\nm thu Mua máy chơi game các loại , ai bán cần thanh lý cần tiền gấp sms 0327494167 nhé zalo viber ..... thu mua cả hồ chí minh , hà nội toàn quốc . tỉnh khác thì ship cod đồng kiểm cho m\\nmua bán nhanh nhẹn mỗi máy chỉ cần test 1-2 phút\\nkhông cần bảo hành trách nhiệm ,sẵn sàng đến tận nơi mua , kể cả máy hỏng hay lỗi đều thu mua nhé\\nSDT của mình : 0327494167', '\\n=======================================================\\nSản phẩm bán chạy trên thị trường hiện nay sử dụng chip thế hệ mới Amlogic S905W là  ở phân khúc giá rẻ bình dân, được trang bị cấu hình mạnh mẽ nên đáp ứng tốt đầy đủ các nhu cầu giải trí như: xem phim độ nét cao (lên đến 4K), xem truyền hình, hát karaoke với đầy đủ các ca khúc, nghe nhạc, đọc báo, chơi game siêu mượt, lướt web,...\\nMáy sở hữu  (chip hỗ trợ âm thanh hình ảnh cực tốt, xử lý ứng dụng mượt) là thế hệ mới nhất của Amlogic.\\nTX3 Mini được trang bị , bộ nhớ trongcho chất lượng đọc ghi nhanh và ở tốc độ cao, khoảng xấp xỉ 20MB/S. Tốc độ Lan tối đa 100Mbps và chuẩn Wifi 1 băng tần thông dụng 2.4G.\\nThông số cấu hình của TV Box TX3 Mini ( Phiên bản Ram 2GB, Rom 16GB )\\nAndroid box TX3 Mini phiên bản 16GB Ram 2GB chip xử lý hàng đầu amlogic S905W\\nVới đầy đủ các cổng kết nối thông dụng của một chiếc Android TV Box, TX3 Mini sở hữu 2 cổng USB 2.0, 1 khay thẻ nhớ TF, 1 cổng LAN RJ45, cổng AV và 1 cổng HDMI. nguồn điện vào máy luôn ở chế độ an toàn chip ROM chuẩn eMMC nên có tốc độ đọc ghi dữ liệu nhanh (gấp 3 lần chuẩn Nand Flash)\\nVới cấu hình cực mạnh, TX3 Mini có thể xử lí tốt các tác vụ giải trí cơ bản của một chiếc TV Box như xem phim, xem truyền hình, hát karaoke, nghe nhạc, đọc báo. lướt web.... hay xa hơn có thể chơi Video 4K ở chất lượng cao nhất một cách mượt mà, chơi mượt các game nặng hiện có của Android điển hình như Asphalt 8, Morden Combat 5...\\nNgoài ra, TX3 Mini cũng được trang bị các chuẩn kết nối không dây tiên tiến nhất như Miracast, Airplay, DLNA... giúp truyền tải nội dung từ thiết bị di động như Smart phone, máy tính bảng lên màn hình TV, giúp việc giải trí của bạn trở nên dễ dàng hơn bao giờ hết.\\nAndroid box TX3 mini, dây HDMI, Điều Khiển, Nguồn.\\nVới việc được trang bị cấu hình và một mức giá như trên, TX3 Mini sẽ là sản phẩm rất đáng để sử dụng và trải nghiệm, nhất là đối với những người tiêu dùng bình dân, khi vấn đề thương hiệu không quá được đặt nặng. Nếu bạn là một người tiêu dùng bình dân thì đây là sản phẩm mà bạn không thể bỏ qua.', 'Bác nào có để lại cho em với ạ']\n",
            "[1, 3, 2, 1, 1, 0, 1, 1, 1, 1]\n",
            "[1, 3, 2, 1, 1, 0, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mseK94bgOWI8"
      },
      "source": [
        "def tokenize(text):\n",
        "  return ViTokenizer.tokenize(text).split()\n",
        "\n",
        "data=[]\n",
        "for i in X:\n",
        "  data.append(tokenize(i))\n",
        "\n",
        "data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XTIy-MHOYyK"
      },
      "source": [
        "word_tokenizer = Tokenizer() \n",
        "word_tokenizer.fit_on_texts(data)  \n",
        "\n",
        "X= word_tokenizer.texts_to_sequences(X)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbFzpgGCOm2z"
      },
      "source": [
        "MAX_SEQ_LENGTH = 100\n",
        "X_padded = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU6hrAazOopT"
      },
      "source": [
        "EMBEDDING_SIZE  = 300  # each word in fast model is represented using a 300 dimensional vector\n",
        "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "#link dowload: https://thiaisotajppub.s3-ap-northeast-1.amazonaws.com/publicfiles/baomoi.window2.vn.model.bin.gz\n",
        "path = '/content/drive/MyDrive/Học tập/Introduce Data Science/word2ve/baomoi.window2.vn.model.bin'\n",
        "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "\n",
        "\n",
        "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
        "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "# create an empty embedding matix\n",
        "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
        "\n",
        "# create a word to index dictionary mapping\n",
        "word2id = word_tokenizer.word_index\n",
        "\n",
        "# copy vectors from word2vec model to the words present in corpus\n",
        "for word, index in word2id.items():\n",
        "    try:\n",
        "        embedding_weights[index, :] = word2vec[word]\n",
        "    except KeyError:\n",
        "        pass"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo5_KnlfOvGU"
      },
      "source": [
        "Y= to_categorical(Y)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi-sOD-KO1rW"
      },
      "source": [
        "rnn_model = Sequential()\n",
        "\n",
        "# create embedding layer - usually the first layer in text problems\n",
        "rnn_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         # vocabulary size - number of unique words in data\n",
        "                        output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
        "                        input_length  =  MAX_SEQ_LENGTH,          # length of input sequence\n",
        "                        weights       = [embedding_weights],      # word embedding matrix\n",
        "                        trainable     =  True                     # True - update the embeddings while training\n",
        "))\n",
        "\n",
        "# add an RNN layer which contains 64 RNN cells\n",
        "rnn_model.add(Bidirectional(LSTM(64, \n",
        "              return_sequences=False  # True - return whole sequence; False - return single output of the end of the sequence\n",
        ")))\n",
        "\n",
        "rnn_model.add(Dense(4, activation='softmax'))"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz-CWXUfO3y-",
        "outputId": "c2cc9153-06fc-44a3-c9a4-d4bfe852c012"
      },
      "source": [
        "rnn_model.summary()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 300)          9052500   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 9,239,896\n",
            "Trainable params: 9,239,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0XfYleKO5Qt"
      },
      "source": [
        "rnn_model.compile(loss      =  'categorical_crossentropy',\n",
        "                  optimizer =  'adam',\n",
        "                  metrics   =  ['acc'])"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiPGDEKXO8Bq",
        "outputId": "fd9219dd-2c48-440c-8cb8-3db39185d31b"
      },
      "source": [
        "rnn_training = rnn_model.fit(X_padded, Y, batch_size=128, epochs=10)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "42/42 [==============================] - 29s 626ms/step - loss: 1.2039 - acc: 0.4740\n",
            "Epoch 2/10\n",
            "42/42 [==============================] - 27s 632ms/step - loss: 0.4914 - acc: 0.8322\n",
            "Epoch 3/10\n",
            "42/42 [==============================] - 27s 633ms/step - loss: 0.2480 - acc: 0.9239\n",
            "Epoch 4/10\n",
            "42/42 [==============================] - 27s 633ms/step - loss: 0.1546 - acc: 0.9504\n",
            "Epoch 5/10\n",
            "42/42 [==============================] - 27s 632ms/step - loss: 0.0940 - acc: 0.9746\n",
            "Epoch 6/10\n",
            "42/42 [==============================] - 27s 635ms/step - loss: 0.0698 - acc: 0.9836\n",
            "Epoch 7/10\n",
            "42/42 [==============================] - 27s 639ms/step - loss: 0.0686 - acc: 0.9832\n",
            "Epoch 8/10\n",
            "42/42 [==============================] - 27s 643ms/step - loss: 0.0425 - acc: 0.9896\n",
            "Epoch 9/10\n",
            "42/42 [==============================] - 27s 640ms/step - loss: 0.0536 - acc: 0.9883\n",
            "Epoch 10/10\n",
            "42/42 [==============================] - 30s 707ms/step - loss: 0.0529 - acc: 0.9845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPzWcBNSO9eq"
      },
      "source": [
        "#chon thu muc can luu\n",
        "rnn_model.save('/content/drive/MyDrive/Học tập/Introduce Data Science/model.h5')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXtkkS-YPvN5",
        "outputId": "197b5487-d207-408d-9ddc-a9c9ae5a8982"
      },
      "source": [
        "test='Điện thoại Iphone10 vừa ra mắt thật đẹp muốn mua mà không có tiền'\n",
        "\n",
        "def embedding(text):\n",
        "  text= ViTokenizer.tokenize(text).split(text)\n",
        "  \n",
        "  text = word_tokenizer.texts_to_sequences(text)\n",
        "  \n",
        "  \n",
        "  text= pad_sequences(text, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "\n",
        "  return text;\n",
        "\n",
        "test=embedding(test)\n",
        "\n",
        "rs=rnn_model.predict(test)\n",
        "rs\n",
        "#ket qua => xac suat thuoc lop 1 la 0.86 => thuoc lop 1"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03137342, 0.8692496 , 0.00534497, 0.09403204]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OzVx7FxMXoeP",
        "outputId": "ddff9595-86b3-484b-f2fa-0b709775434d"
      },
      "source": [
        "# lay nhan cua lop 1\n",
        "numToLabel(1)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Điện thoại di động'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    }
  ]
}